{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - the goal of the Markov Chain\n",
    "\n",
    "we are building a third-order markov model.\n",
    "\n",
    "to do this, we take the input stream of tokens and capture each trigram.\n",
    "```\n",
    "e.g.  \n",
    "(a,b,c,d,a,b,c,a,b,e,d,a) ->\n",
    "(a,b,c), (b,c,d), (c,d,a), (d,a,b), (a,b,c), (b,c,a), (c,a,b), (a,b,e), (b,e,d), (e,d,a)\n",
    "0        1        2        3        4        5        6        7        8        9\n",
    "```\n",
    "imagine a cup for each unique pair of tokens starting a trigram\n",
    "\n",
    "in the cup, put the third letter of the trigram\n",
    "\n",
    "```\n",
    "(a,b) -> 0:(a,b,c), 4:(a,b,c), 7:(a,b,e)\n",
    "```\n",
    "\n",
    "so cup (a,b) gets 2 (c) and 1 (e)\n",
    "\n",
    "we start in a random state.\n",
    "\n",
    "say we start in state (a,b)\n",
    "\n",
    "from here, there is a 66% chance we choose (c) next and a 33% chance we choose (e) next\n",
    "\n",
    "if we choose (c), follow these steps:\n",
    "    output the (a)\n",
    "    create a new bigram from the second element of the state's key (b) plus the choice: (b,c)\n",
    "    move to state (b,c)\n",
    "    probabalistically choose an element from state (b,c)\n",
    "    repeat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'a', 'b', 'c', 'a', 'b', 'e', 'd', 'a']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "text = \"a b c d a b c a b e d a\"\n",
    "\n",
    "counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# note we will use nltk sentence tokenizer for real text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - create token trigrams\n",
    "\n",
    "we use zip() to accomplish this.\n",
    "\n",
    "note, in our system, Ramona, we are building our trigrams out of sentences so that we can generate sentences by modelling the beginning and endings of sentences.  If we wanted to process chunks of text larger than a sentence, which could potentially overwhelm the in-memory lists and zip, we would need to process the tokens in batches, and keep track of the last n-2 tokens in each batch and add them to the beginning of the subsequent batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the trigrams\n",
      "('a', 'b', 'c')\n",
      "('a', 'b', 'c')\n",
      "('a', 'b', 'e')\n",
      "('b', 'c', 'a')\n",
      "('b', 'c', 'd')\n",
      "('b', 'e', 'd')\n",
      "('c', 'a', 'b')\n",
      "('c', 'd', 'a')\n",
      "('d', 'a', 'b')\n",
      "('e', 'd', 'a')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigrams = sorted(list(zip(tokens, tokens[1:], tokens[2:])))  # all dimensions get sorted\n",
    "\n",
    "print(\"all the trigrams\")\n",
    "for t in trigrams:\n",
    "    print(t)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Count the continuation element for each bigram state\n",
    "\n",
    "make a state for each bigram in our corpus.  \n",
    "\n",
    "for each bigram state, we count the number of tokens that come after the bigram.  as we advance through the machine, we will move to a new state by bigram and then select a next 1-gram based on probabilities.\n",
    "\n",
    "for example, if the bigram (a,b) is followed by (c) 3 times and followed by (d) 1 time, we would capture these counts here.\n",
    "\n",
    "later these counts will be turned into a probability distribution to run the machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gather up the counts\n",
      "('a', 'b') -> [('a', 'b', 'c'), ('a', 'b', 'c'), ('a', 'b', 'e')]\n",
      "   c -> 2\n",
      "   e -> 1\n",
      "\n",
      "('b', 'c') -> [('b', 'c', 'a'), ('b', 'c', 'd')]\n",
      "   a -> 1\n",
      "   d -> 1\n",
      "\n",
      "('b', 'e') -> [('b', 'e', 'd')]\n",
      "   d -> 1\n",
      "\n",
      "('c', 'a') -> [('c', 'a', 'b')]\n",
      "   b -> 1\n",
      "\n",
      "('c', 'd') -> [('c', 'd', 'a')]\n",
      "   a -> 1\n",
      "\n",
      "('d', 'a') -> [('d', 'a', 'b')]\n",
      "   b -> 1\n",
      "\n",
      "('e', 'd') -> [('e', 'd', 'a')]\n",
      "   a -> 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {('a', 'b'): defaultdict(int, {'c': 2, 'e': 1}),\n",
       "             ('b', 'c'): defaultdict(int, {'a': 1, 'd': 1}),\n",
       "             ('b', 'e'): defaultdict(int, {'d': 1}),\n",
       "             ('c', 'a'): defaultdict(int, {'b': 1}),\n",
       "             ('c', 'd'): defaultdict(int, {'a': 1}),\n",
       "             ('d', 'a'): defaultdict(int, {'b': 1}),\n",
       "             ('e', 'd'): defaultdict(int, {'a': 1})})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "print(\"now gather up the counts\")\n",
    "for k, g in groupby(trigrams, lambda x: (x[0], x[1])):\n",
    "    # for production version, we would not make this a list here\n",
    "    g = list(g)\n",
    "    print(f'{k} -> {g}')\n",
    "    for k2, g2 in groupby(g, lambda x: x[2]):\n",
    "        # for production version, we would not make this a list here\n",
    "        g2 = list(g2)\n",
    "        print(f'   {k2} -> {len(g2)}')\n",
    "        counts[k][k2] += len(list(g2))\n",
    "    print()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - convert counts to probabilities\n",
    "\n",
    "for each bigram state, we create a tuple of two parallel lists.\n",
    "\n",
    "one list contains the list of continuation tokens\n",
    "\n",
    "one list contains the list of probabilities for that continuation token\n",
    "\n",
    "at the end of this step, we have our model, and we can use this model to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'): (['c', 'e'], [0.6666666666666666, 0.3333333333333333]),\n",
       " ('b', 'c'): (['a', 'd'], [0.5, 0.5]),\n",
       " ('b', 'e'): (['d'], [1.0]),\n",
       " ('c', 'a'): (['b'], [1.0]),\n",
       " ('c', 'd'): (['a'], [1.0]),\n",
       " ('d', 'a'): (['b'], [1.0]),\n",
       " ('e', 'd'): (['a'], [1.0])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = {}\n",
    "for k,v in counts.items():\n",
    "    n = sum(v.values())\n",
    "    words=[]\n",
    "    probs=[]\n",
    "    for k2,v2 in v.items():\n",
    "        words.append(k2)\n",
    "        probs.append(v2/n)\n",
    "    states[k] = (words, probs)\n",
    "\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - generate output based on probabilities in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly choose initial state\n",
      "curr_state: ('d', 'a') -> (['b'], [1.0])\n",
      "append: (d)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (c)\n",
      "curr_state: ('b', 'c') -> (['a', 'd'], [0.5, 0.5])\n",
      "append: (b)\n",
      "pick: (a)\n",
      "curr_state: ('c', 'a') -> (['b'], [1.0])\n",
      "append: (c)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (c)\n",
      "curr_state: ('b', 'c') -> (['a', 'd'], [0.5, 0.5])\n",
      "append: (b)\n",
      "pick: (d)\n",
      "curr_state: ('c', 'd') -> (['a'], [1.0])\n",
      "append: (c)\n",
      "pick: (a)\n",
      "curr_state: ('d', 'a') -> (['b'], [1.0])\n",
      "append: (d)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (c)\n",
      "curr_state: ('b', 'c') -> (['a', 'd'], [0.5, 0.5])\n",
      "append: (b)\n",
      "pick: (a)\n",
      "curr_state: ('c', 'a') -> (['b'], [1.0])\n",
      "append: (c)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (c)\n",
      "curr_state: ('b', 'c') -> (['a', 'd'], [0.5, 0.5])\n",
      "append: (b)\n",
      "pick: (d)\n",
      "curr_state: ('c', 'd') -> (['a'], [1.0])\n",
      "append: (c)\n",
      "pick: (a)\n",
      "curr_state: ('d', 'a') -> (['b'], [1.0])\n",
      "append: (d)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (c)\n",
      "curr_state: ('b', 'c') -> (['a', 'd'], [0.5, 0.5])\n",
      "append: (b)\n",
      "pick: (d)\n",
      "curr_state: ('c', 'd') -> (['a'], [1.0])\n",
      "append: (c)\n",
      "pick: (a)\n",
      "curr_state: ('d', 'a') -> (['b'], [1.0])\n",
      "append: (d)\n",
      "pick: (b)\n",
      "curr_state: ('a', 'b') -> (['c', 'e'], [0.6666666666666666, 0.3333333333333333])\n",
      "append: (a)\n",
      "pick: (e)\n",
      "['d', 'a', 'b', 'c', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "out = []\n",
    "not_done = True\n",
    "print(\"randomly choose initial state\")\n",
    "curr_state = random.choice(list(states.keys()))\n",
    "MAX_ITERS = 20\n",
    "iter = 0\n",
    "while iter < MAX_ITERS:\n",
    "    print(f\"curr_state: {curr_state} -> {states[curr_state]}\")\n",
    "    print(f\"append: ({curr_state[0]})\")\n",
    "    out.append(curr_state[0])\n",
    "    randompick = np.random.multinomial(1, states[curr_state][1])\n",
    "    idx = list(randompick).index(1)\n",
    "    print(f\"pick: ({states[curr_state][0][idx]})\")\n",
    "    curr_state = (curr_state[1], states[curr_state][0][idx])\n",
    "    iter += 1\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Conclusion\n",
    "\n",
    "This is the algorithm implemented in Ramona.\n",
    "\n",
    "See the code in model.py for the productionized version.  \n",
    "It is not very different, but does some extra work around sentence boundaries in order to generate natural sounding sentences.\n",
    "\n",
    "Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
